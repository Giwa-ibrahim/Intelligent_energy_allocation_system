{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84025361",
   "metadata": {},
   "source": [
    "## Import all libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5c9e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'IPython'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.gridspec as gridspec\n",
    "import shap\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, root_mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49829154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Data\n",
    "data= pd.read_excel('data.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04707038",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [c.strip().lower() for c in data.columns]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['datetime'] = pd.to_datetime(data['datetime'], errors='coerce')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert each feeders from WH to MWH\n",
    "for col in data.columns[1:]:  # Start from the second column\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce') \n",
    "    data[col] = data[col] / 1e6\n",
    "        \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d3749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Linear interpolation to fill missing values\n",
    "data_clean = data.copy()\n",
    "data_clean = data_clean.set_index('datetime')\n",
    "# Interpolate missing values\n",
    "data_clean = data_clean.interpolate(method='time')\n",
    "data_clean = data_clean.reset_index()\n",
    "# Check remaining missing values\n",
    "data_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccbfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d947e2",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.copy()\n",
    "data_clean['hour'] = data_clean['datetime'].dt.hour\n",
    "data_clean['day_of_week'] = data_clean['datetime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "data_clean['month'] = data_clean['datetime'].dt.month\n",
    "data_clean['day'] = data_clean['datetime'].dt.day\n",
    "data_clean['year'] = data_clean['datetime'].dt.year\n",
    "data_clean['is_weekend'] = (data_clean['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b895c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create cyclical features for time variables (preserves cyclical nature)\n",
    "data_clean['hour_sin'] = np.sin(2 * np.pi * data_clean['hour']/24)\n",
    "data_clean['hour_cos'] = np.cos(2 * np.pi * data_clean['hour']/24)\n",
    "data_clean['month_sin'] = np.sin(2 * np.pi * data_clean['month']/12)\n",
    "data_clean['month_cos'] = np.cos(2 * np.pi * data_clean['month']/12)\n",
    "data_clean['day_of_week_sin'] = np.sin(2 * np.pi * data_clean['day_of_week']/7)\n",
    "data_clean['day_of_week_cos'] = np.cos(2 * np.pi * data_clean['day_of_week']/7)\n",
    "\n",
    "data_clean.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all columns\n",
    "all_columns = data_clean.columns.tolist()\n",
    "\n",
    "# Identify feeder columns (all numeric columns except the engineered features)\n",
    "time_feature_cols = ['hour', 'day_of_week', 'month', 'day', 'year', 'is_weekend', \n",
    "                     'hour_sin', 'hour_cos', 'month_sin', 'month_cos', \n",
    "                     'day_of_week_sin', 'day_of_week_cos']\n",
    "\n",
    "# First column is datetime, the rest are feeder columns initially\n",
    "feeder_cols = [col for col in all_columns if col not in ['datetime'] + time_feature_cols]\n",
    "\n",
    "# Create new column order: datetime, time features, then feeders\n",
    "new_column_order = ['datetime'] + time_feature_cols + feeder_cols\n",
    "\n",
    "# Reorder the DataFrame\n",
    "data_clean = data_clean[new_column_order]\n",
    "\n",
    "# Verify the new order\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3628b",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cff52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for storing plots if it doesn't exist\n",
    "plot_dir = 'energy_plots'\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "\n",
    "# Create a copy for plotting\n",
    "plot_data = data_clean.copy()\n",
    "\n",
    "# 1. Daily consumption patterns by hour\n",
    "plt.figure(figsize=(10, 6))\n",
    "hourly_consumption = plot_data.groupby('hour')[feeder_cols].mean()\n",
    "hourly_consumption.mean(axis=1).plot()\n",
    "plt.title('Average Hourly Energy Consumption Pattern')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Average Energy (MWH)')\n",
    "plt.xticks(range(0, 24, 2))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plot_dir, 'hourly_consumption.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 2. Weekly patterns by day\n",
    "plt.figure(figsize=(10, 6))\n",
    "daily_consumption = plot_data.groupby('day_of_week')[feeder_cols].mean()\n",
    "daily_consumption.mean(axis=1).plot()\n",
    "plt.title('Average Daily Energy Consumption Pattern')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Energy (MWH)')\n",
    "plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plot_dir, 'daily_consumption.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 3. Monthly seasonal patterns\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_consumption = plot_data.groupby('month')[feeder_cols].mean()\n",
    "monthly_consumption.mean(axis=1).plot()\n",
    "plt.title('Monthly Energy Consumption Pattern')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Energy (MWH)')\n",
    "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plot_dir, 'monthly_consumption.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 4. Weekday vs Weekend comparison\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# weekend_comparison = plot_data.groupby(['is_weekend', 'hour'])[feeder_cols].mean().mean(axis=1).unstack()\n",
    "# weekend_comparison.plot()\n",
    "# plt.title('Weekday vs Weekend Energy Consumption')\n",
    "# plt.xlabel('Hour of Day')\n",
    "# plt.ylabel('Average Energy (MWH)')\n",
    "# plt.legend(['Weekday', 'Weekend'])\n",
    "# plt.xticks(range(0, 24, 2))\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(plot_dir, 'weekend_comparison.png'), dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# 5. Correlation heatmap between feeders\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation = plot_data[feeder_cols].corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Between Energy Feeders')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plot_dir, 'feeder_correlation.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 6. Time series plot of total consumption\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Sample the data to avoid overcrowding (e.g., daily averages)\n",
    "daily_data = plot_data.set_index('datetime').resample('D').mean()\n",
    "total_energy = daily_data[feeder_cols].sum(axis=1)\n",
    "total_energy.plot()\n",
    "plt.title('Total Energy Consumption Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Energy (MWH)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plot_dir, 'total_consumption_trend.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 7. Boxplot of hourly consumption patterns\n",
    "plt.figure(figsize=(15, 8))\n",
    "hourly_data = plot_data.melt(id_vars=['hour'], \n",
    "                          value_vars=feeder_cols,\n",
    "                          var_name='Feeder', \n",
    "                          value_name='Energy')\n",
    "sns.boxplot(x='hour', y='Energy', data=hourly_data)\n",
    "plt.title('Distribution of Energy Consumption by Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Energy (MWH)')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plot_dir, 'hourly_distribution.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 8. Top consumers comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "feeder_means = plot_data[feeder_cols].mean().sort_values(ascending=False)\n",
    "feeder_means.plot(kind='bar')\n",
    "plt.title('Average Energy Consumption by Feeder')\n",
    "plt.xlabel('Feeder')\n",
    "plt.ylabel('Average Energy (MWH)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plot_dir, 'feeder_comparison.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 9. Time series decomposition for the largest consumer\n",
    "# top_feeder = feeder_means.index[0]\n",
    "# # Resample to daily data for clearer decomposition\n",
    "# daily_series = plot_data.set_index('datetime')[top_feeder].resample('D').mean()\n",
    "# # Fill missing values if any\n",
    "# daily_series = daily_series.fillna(daily_series.mean())\n",
    "\n",
    "# # Perform decomposition\n",
    "# decomposition = seasonal_decompose(daily_series, model='additive', period=7)  # 7 days per week\n",
    "\n",
    "# # Plot the decomposition components individually\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(daily_series)\n",
    "# plt.title(f'Original Time Series: {top_feeder}')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Energy (MWH)')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(plot_dir, 'decomp_original.png'), dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(decomposition.trend)\n",
    "# plt.title(f'Trend Component: {top_feeder}')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Energy (MWH)')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(plot_dir, 'decomp_trend.png'), dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(decomposition.seasonal)\n",
    "# plt.title(f'Seasonal Component: {top_feeder}')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Energy (MWH)')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(plot_dir, 'decomp_seasonal.png'), dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(decomposition.resid)\n",
    "# plt.title(f'Residual Component: {top_feeder}')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Energy (MWH)')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(plot_dir, 'decomp_residual.png'), dpi=300)\n",
    "#plt.show()\n",
    "\n",
    "print(f\"All plots have been saved to the '{plot_dir}' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d78fa",
   "metadata": {},
   "source": [
    "## Prepare Data for Hourly Time-Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1706f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 24  # Use 24 hours of history for prediction\n",
    "forecast_horizon = 24  # Predict the next 24 hours\n",
    "batch_size = 32\n",
    "\n",
    "data_model = data_clean.copy()\n",
    "print(f\"Data shape: {data_model.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c67691",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = data_model[time_feature_cols].values\n",
    "y_raw = data_model[feeder_cols].values\n",
    "print(f\"X shape: {X_raw.shape}, y shape: {y_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29691e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = feature_scaler.fit_transform(X_raw)\n",
    "y_scaled = target_scaler.fit_transform(y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deef2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_length, forecast_length):\n",
    "    \"\"\"Create input/output sequences for LSTM training from a dataframe\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - seq_length - forecast_length + 1):\n",
    "        # Input sequence: previous seq_length time steps\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        # Target sequence: next forecast_length time steps\n",
    "        y_seq.append(y[i+seq_length:i+seq_length+forecast_length])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequences, y_sequences = create_sequences(X_scaled, y_scaled, sequence_length, forecast_horizon)\n",
    "print(f\"Created sequences - X: {X_sequences.shape}, y: {y_sequences.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af148a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For time series, keep data in order (no shuffling)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, shuffle=False, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12034ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Testing data: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed15f96c",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44522dfc",
   "metadata": {},
   "source": [
    "### LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ad295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Get the dimensions from our training data\n",
    "n_features = X_train.shape[2]  # Number of input features\n",
    "n_outputs = y_train.shape[2]   # Number of target variables (feeders)\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_lstm_model(sequence_length, n_features, n_outputs, forecast_horizon):\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(sequence_length, n_features)),\n",
    "        \n",
    "        # LSTM layers with dropout to prevent overfitting\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.LSTM(64),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Dense layers to interpret LSTM outputs\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        \n",
    "        # Reshape to match the output dimensions\n",
    "        layers.Dense(n_outputs * forecast_horizon),\n",
    "        layers.Reshape((forecast_horizon, n_outputs))\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',  \n",
    "        metrics=['mae', 'mape']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff37681",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = create_lstm_model(\n",
    "    sequence_length=sequence_length,\n",
    "    n_features=n_features,\n",
    "    n_outputs=n_outputs,\n",
    "    forecast_horizon=forecast_horizon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400387e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'models/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Set up callbacks for training\n",
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=0.0001\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, 'best_energy_forecast_model.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    callbacks.TensorBoard(\n",
    "        log_dir='./logs',\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685dfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving scalers to disk...\")\n",
    "joblib.dump(feature_scaler, os.path.join(model_dir, 'feature_scaler.pkl'))\n",
    "joblib.dump(target_scaler, os.path.join(model_dir, 'target_scaler.pkl'))\n",
    "print(f\"Scalers saved to {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,  # Use 20% of training data as validation\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_mae, test_mape = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Test MAPE: {test_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc71698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "train_plot_dir = 'training_plots/'\n",
    "if not os.path.exists(train_plot_dir):\n",
    "    os.makedirs(train_plot_dir)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(train_plot_dir,'training_history.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8aa94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save(os.path.join(model_dir, 'lstm_model.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions and actual values to original scale\n",
    "y_test_orig = target_scaler.inverse_transform(y_test.reshape(-1, n_outputs)).reshape(y_test.shape)\n",
    "y_pred_orig = target_scaler.inverse_transform(y_pred.reshape(-1, n_outputs)).reshape(y_pred.shape)\n",
    "\n",
    "# Calculate error metrics on original scale\n",
    "mae = mean_absolute_error(y_test_orig.reshape(-1, n_outputs), y_pred_orig.reshape(-1, n_outputs))\n",
    "mape = mean_absolute_percentage_error(y_test_orig.reshape(-1, n_outputs), y_pred_orig.reshape(-1, n_outputs))\n",
    "rmse = root_mean_squared_error(y_test_orig.reshape(-1, n_outputs), y_pred_orig.reshape(-1, n_outputs))\n",
    "\n",
    "print(f\"Mean Absolute Error (in original scale): {mae:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mape*100:.2f}%\")\n",
    "print(f\"Root Mean Squared Error (in original scale): {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeaa4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = 'prediction_plots/'\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "# Plot predictions vs actual for the first test sample\n",
    "sample_idx = 0\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "# Plot for each feeder (first 4 for clarity)\n",
    "for i in range(min(4, n_outputs)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.plot(y_test_orig[sample_idx, :, i], label='Actual')\n",
    "    plt.plot(y_pred_orig[sample_idx, :, i], label='Predicted')\n",
    "    plt.title(f'Feeder {feeder_cols[i]}')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Energy (MWh)')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(pred_dir, 'sample_predictions.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot total energy consumption (sum of all feeders)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.sum(y_test_orig[sample_idx], axis=1), label='Actual Total')\n",
    "plt.plot(np.sum(y_pred_orig[sample_idx], axis=1), label='Predicted Total')\n",
    "plt.title('Total Energy Consumption - Actual vs Predicted')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Total Energy (MWh)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(pred_dir, 'total_prediction.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd67d4f",
   "metadata": {},
   "source": [
    "## SHAP Explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a section for SHAP Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"SHAP ANALYSIS: EXPLAINING MODEL PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "timestep_explain= 0\n",
    "\n",
    "def model_predict(X):\n",
    "    # Convert to the shape expected by the LSTM (adding batch and sequence dimensions if needed)\n",
    "    samples= X.shape[0]\n",
    "    X_reshaped = X.reshape(samples, sequence_length, n_features)\n",
    "    preds = model.predict(X_reshaped, verbose=1)\n",
    "    return preds[:, timestep_explain, :]\n",
    "\n",
    "background_data = 50  # Use 50 examples as background\n",
    "background_samples = X_train[:background_data]\n",
    "\n",
    "#flatten the background samples\n",
    "background_samples = background_samples.reshape(background_data, -1)\n",
    "print(f\"Background data shape: {background_samples.shape}\")\n",
    "\n",
    "# Initialize the SHAP explainer with the model and background data\n",
    "explainer = shap.KernelExplainer(model_predict, background_samples, link=\"identity\")\n",
    "\n",
    "# Choose a test sample to explain\n",
    "sample = X_test[sample_idx:sample_idx+1]  # Recall sample_idx= 0\n",
    "#print(f\"Sample shape for SHAP: {sample.shape}\")\n",
    "sample_flat = sample.reshape(1, -1)\n",
    "print(f\"Sample shape for SHAP: {sample_flat.shape}\")\n",
    "\n",
    "# Calculate SHAP values for this sample\n",
    "shap_values = explainer.shap_values(sample_flat)\n",
    "\n",
    "# Print the shape of SHAP values to understand the structure\n",
    "print(f\"SHAP values shape: {np.array(shap_values).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37207b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1038d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature names for visualization\n",
    "# We'll create feature names like 'hour_t0', 'day_of_week_t0', etc.\n",
    "basic_feature_names = [\n",
    "    \"hour\", \"day_of_week\", \"month\", \"day\", \"year\", \"is_weekend\",\n",
    "    \"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\",\n",
    "    \"day_of_week_sin\", \"day_of_week_cos\"\n",
    "]\n",
    "\n",
    "feature_names = []\n",
    "for t in range(sequence_length):\n",
    "    for f in basic_feature_names:\n",
    "        feature_names.append(f\"{f}_t{t}\")\n",
    "\n",
    "# Plot summary for each feeder\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i in range(min(4, n_outputs)):  # Just plot first 4 feeders\n",
    "    plt.subplot(2, 2, i+1)\n",
    "\n",
    "\n",
    "    if shap_values.shape[0] != sample_flat.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Mismatch for output {i}: SHAP values have shape {shap_values.shape} \"\n",
    "            f\"but explain_samples have shape {sample_flat.shape}\"\n",
    "        )\n",
    "\n",
    "    # For summary plot, we limit to top 20 features to keep it readable\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        sample_flat,\n",
    "        feature_names=feature_names,\n",
    "        max_display=20,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"Feature Importance for {feeder_cols[i]} Prediction\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(pred_dir, 'shap_feature_importance.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Create a more detailed visualization for the first feeder\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Get the most important features for the first feeder\n",
    "feature_importance = np.abs(shap_values[0].mean(0))\n",
    "top_indices = np.argsort(-feature_importance)[:10]  # Top 10 features\n",
    "top_names = [feature_names[i] for i in top_indices]\n",
    "\n",
    "# Create a bar chart of the most important features\n",
    "plt.barh(range(len(top_names)), [feature_importance[i] for i in top_indices])\n",
    "plt.yticks(range(len(top_names)), top_names)\n",
    "plt.title(f\"Top 10 Important Features for {feeder_cols[0]}\")\n",
    "plt.xlabel(\"Mean |SHAP Value|\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(pred_dir, 'top_features.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"SHAP analysis completed. Results saved to the 'prediction_plots' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6016211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of feature indices to feature names for better interpretability\n",
    "feature_names = [\n",
    "    \"hour\", \"day_of_week\", \"month\", \"day\", \"year\", \"is_weekend\",\n",
    "    \"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\",\n",
    "    \"day_of_week_sin\", \"day_of_week_cos\"\n",
    "]\n",
    "\n",
    "# Plot force plots for the first feeder to explain individual predictions in detail\n",
    "plt.figure(figsize=(15, 8))\n",
    "shap.initjs()  # Initialize JavaScript visualization\n",
    "\n",
    "# Create a force plot for the first feeder\n",
    "force_plot = shap.force_plot(\n",
    "    explainer.expected_value[0],\n",
    "    shap_values[0][0],\n",
    "    sample_flat[0],\n",
    "    feature_names=feature_names,\n",
    "    matplotlib=True,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "plt.title(f\"Detailed SHAP Explanation for {feeder_cols[0]} Prediction\")\n",
    "plt.savefig(os.path.join(pred_dir, 'shap_force_plot.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Create a dependence plot to show how a specific feature affects predictions\n",
    "# For example, showing how hour_sin affects predictions for the first feeder\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.dependence_plot(\n",
    "    \"hour_sin\", \n",
    "    shap_values[0], \n",
    "    sample_flat,\n",
    "    feature_names=feature_names,\n",
    "    show=False\n",
    ")\n",
    "plt.title(f\"How Hour (Sine Transform) Affects {feeder_cols[0]} Predictions\")\n",
    "plt.savefig(os.path.join(pred_dir, 'shap_dependence_plot.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Time series specific analysis\n",
    "# Create a plot showing SHAP values across time for specific features\n",
    "# First, we need to generate SHAP values for a sequence of predictions\n",
    "sequence_to_explain = X_test[sample_idx:sample_idx+1]\n",
    "sequence_shap_values = explainer.shap_values(sequence_to_explain)\n",
    "\n",
    "# Plot SHAP values for time-related features for the first feeder\n",
    "time_feature_indices = [0, 6, 7]  # hour, hour_sin, hour_cos\n",
    "time_feature_names = [feature_names[i] for i in time_feature_indices]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, idx in enumerate(time_feature_indices):\n",
    "    plt.subplot(len(time_feature_indices), 1, i+1)\n",
    "    plt.bar(range(sequence_length), sequence_shap_values[0][0, idx])\n",
    "    plt.title(f\"SHAP Values for {time_feature_names[i]} Over Time\")\n",
    "    plt.ylabel(\"SHAP Value\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(pred_dir, 'shap_time_features.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"SHAP analysis completed. Results saved to the 'prediction_plots' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dae24f",
   "metadata": {},
   "source": [
    "## Feeder Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ff338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformer constraints\n",
    "TRANSFORMERS = {\n",
    "    'T1': {'capacity': 40, 'feeders': ['sango']},\n",
    "    'T2': {'capacity': 60, 'feeders': ['fsm', 'amje', 'sumo']},\n",
    "    'T3': {'capacity': 100, 'feeders': ['tower']},\n",
    "    'T4': {'capacity': 60, 'feeders': ['qualitec', 'arrachid', 'ijagba', 'tollgate', 'aarti']},\n",
    "    'T5': {'capacity': 40, 'feeders': ['idiroko', 'estate']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782110fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority tiers\n",
    "TIER1 = {\n",
    "    'SECURITY': ['sango', 'tollgate']          # Security critical\n",
    "}\n",
    "TIER2 = {\n",
    "    'HEALTHCARE': ['idiroko']                    # Healthcare critical                        \n",
    "}\n",
    "TIER3 = {\n",
    "    'FINANCIAL': ['qualitec', 'aarti', 'tower', 'arrachid', 'sumo']  # Financial services are Dedicated Lines\n",
    "}\n",
    "TIER4 = {\n",
    "    'GENERAL': ['ijagba', 'amje', 'estate']        # General services; Residential, Industrial and Commercial\n",
    "}\n",
    "\n",
    "# Flattened tiers for easier processing\n",
    "PRIORITY_ORDER = [\n",
    "    *TIER1['SECURITY'],    # Security critical (first)\n",
    "    *TIER2['HEALTHCARE'],                         # Healthcare critical (second)\n",
    "    *TIER3['FINANCIAL'],                        # Financial services (third)\n",
    "    *TIER4['GENERAL']                            # General purpose (last)\n",
    "]\n",
    "PRIORITY_TIERS = [TIER1, TIER2, TIER3, TIER4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40377be",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIORITY_TIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIORITY_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_energy(forecasted_demand, available_supply):\n",
    "    \"\"\"\n",
    "    Allocate available energy supply to feeders based on priority tiers\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    forecasted_demand : dict\n",
    "        Dictionary mapping feeder names to their forecasted demand in MW\n",
    "    available_supply : float\n",
    "        Total available energy supply in MW\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping feeder names to allocated energy in MW\n",
    "    \"\"\"\n",
    "    total_demand = sum(forecasted_demand.values())\n",
    "    allocation = {feeder: 0 for feeder in forecasted_demand}\n",
    "    \n",
    "    # Case 1: Supply exceeds or equals demand\n",
    "    if available_supply >= total_demand:\n",
    "        print(f\"Supply ({available_supply:.2f} MW) exceeds demand ({total_demand:.2f} MW)\")\n",
    "        print(\"All feeders will receive 100% of their requested demand\")\n",
    "        return forecasted_demand\n",
    "    \n",
    "    # Case 2: Demand exceeds supply - allocate by priority\n",
    "    print(f\"Demand ({total_demand:.2f} MW) exceeds supply ({available_supply:.2f} MW)\")\n",
    "    print(\"Allocating based on priority tiers...\")\n",
    "    \n",
    "    remaining_supply = available_supply\n",
    "    \n",
    "    # Allocate by priority order\n",
    "    for feeder in PRIORITY_ORDER:\n",
    "        # Skip if this feeder is not in our forecast\n",
    "        if feeder not in forecasted_demand:\n",
    "            continue\n",
    "            \n",
    "        feeder_demand = forecasted_demand[feeder]\n",
    "        \n",
    "        # If we have enough supply for this feeder\n",
    "        if remaining_supply >= feeder_demand:\n",
    "            allocation[feeder] = feeder_demand\n",
    "            remaining_supply -= feeder_demand\n",
    "            print(f\"  Allocated {feeder_demand:.2f} MW to {feeder} (100% of demand)\")\n",
    "        \n",
    "        # If we have some supply but not enough for full demand\n",
    "        elif remaining_supply > 0:\n",
    "            allocation[feeder] = remaining_supply\n",
    "            print(f\"  Allocated {remaining_supply:.2f} MW to {feeder} ({remaining_supply/feeder_demand*100:.1f}% of demand)\")\n",
    "            remaining_supply = 0\n",
    "        \n",
    "        # If we're out of supply\n",
    "        else:\n",
    "            print(f\" No supply left for {feeder}\")\n",
    "    \n",
    "    return allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9273b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_allocation(allocation, forecasted_demand):\n",
    "    \"\"\"Analyze and report on the energy allocation\"\"\"\n",
    "    total_allocated = sum(allocation.values())\n",
    "    total_demand = sum(forecasted_demand.values())\n",
    "    \n",
    "    print(\"\\nAllocation Analysis:\")\n",
    "    print(f\"Total Demand: {total_demand:.2f} MW\")\n",
    "    print(f\"Total Allocated: {total_allocated:.2f} MW\")\n",
    "    print(f\"Allocation Rate: {total_allocated/total_demand*100:.1f}%\")\n",
    "    \n",
    "    # Analyze by tier\n",
    "    for tier_name, tier_dict in [(\"TIER1 (Critical)\", TIER1), \n",
    "                               (\"TIER2 (Financial)\", TIER2),\n",
    "                               (\"TIER3 (Industrial)\", TIER3), \n",
    "                               (\"TIER4 (General)\", TIER4)]:\n",
    "        tier_feeders = [f for sublist in tier_dict.values() for f in sublist]\n",
    "        tier_demand = sum(forecasted_demand.get(f, 0) for f in tier_feeders)\n",
    "        tier_allocated = sum(allocation.get(f, 0) for f in tier_feeders)\n",
    "        \n",
    "        if tier_demand > 0:\n",
    "            print(f\"\\n{tier_name}:\")\n",
    "            print(f\"  Demand: {tier_demand:.2f} MW\")\n",
    "            print(f\"  Allocated: {tier_allocated:.2f} MW\")\n",
    "            print(f\"  Satisfaction Rate: {tier_allocated/tier_demand*100:.1f}%\")\n",
    "            \n",
    "    # Check transformer constraints\n",
    "    print(\"\\nTransformer Loading Analysis:\")\n",
    "    for tx_name, tx_info in TRANSFORMERS.items():\n",
    "        tx_capacity = tx_info['capacity']\n",
    "        tx_feeders = tx_info['feeders']\n",
    "        tx_allocation = sum(allocation.get(f, 0) for f in tx_feeders)\n",
    "        tx_utilization = tx_allocation / tx_capacity * 100\n",
    "        \n",
    "        print(f\"Transformer {tx_name} ({tx_capacity} MW capacity):\")\n",
    "        print(f\"  Feeders: {', '.join(tx_feeders)}\")\n",
    "        print(f\"  Allocated: {tx_allocation:.2f} MW\")\n",
    "        print(f\"  Utilization: {tx_utilization:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_year_forecast(model, feature_scaler, target_scaler, initial_data, n_features, n_outputs, forecast_days=30):\n",
    "    \"\"\"\n",
    "    Generate a one-year ahead forecast using the trained LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Trained LSTM model\n",
    "    feature_scaler : MinMaxScaler\n",
    "        Scaler used for features\n",
    "    target_scaler : MinMaxScaler\n",
    "        Scaler used for targets\n",
    "    initial_data : np.array\n",
    "        Initial input data for the model (must be of shape [1, sequence_length, n_features])\n",
    "    n_features : int\n",
    "        Number of input features\n",
    "    n_outputs : int\n",
    "        Number of outputs (number of feeders)\n",
    "    forecast_days : int\n",
    "        Number of days to forecast (default: 365)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    np.array\n",
    "        Forecasted values for each feeder for each day in original scale\n",
    "    \"\"\"\n",
    "    # Initialize containers for forecast\n",
    "    forecast_hours = forecast_days * 24  # 24 hours per day\n",
    "    predictions = np.zeros((forecast_hours, n_outputs))\n",
    "    \n",
    "    # Make a copy of the initial input data\n",
    "    current_input = initial_data.copy()\n",
    "    \n",
    "    # Generate forecasts one day at a time\n",
    "    for hour in range(forecast_hours):\n",
    "        # Generate forecast for next hour\n",
    "        next_hour_forecast = model.predict(current_input)\n",
    "        \n",
    "        # Store the forecast for this hour\n",
    "        predictions[hour] = next_hour_forecast[0, 0, :]\n",
    "        \n",
    "        # Update the input for the next prediction\n",
    "        # Shift the window forward by one step and append the new prediction\n",
    "        current_input[0, :-1, :] = current_input[0, 1:, :]\n",
    "        \n",
    "        # Create new features for the next time step (this would need customization based on your features)\n",
    "        # This is a simplified version that assumes your features include hour of day, day of week, etc.\n",
    "        # For a real implementation, you'd need to update all time-based features correctly\n",
    "        # For now, we'll just use the last row's features\n",
    "        current_input[0, -1, :] = current_input[0, -1, :]\n",
    "    \n",
    "    # Convert predictions back to original scale\n",
    "    predictions_orig = target_scaler.inverse_transform(predictions)\n",
    "    \n",
    "    return predictions_orig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb067e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate the allocation system with different supply scenarios\n",
    "def run_allocation_simulation(forecasted_demand, base_demand_total, variation_percentages=[-20, -10, 0, 10, 20]):\n",
    "    \"\"\"\n",
    "    Run energy allocation simulations for different supply scenarios\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    forecasted_demand : dict\n",
    "        Dictionary mapping feeder names to their forecasted demand in MW\n",
    "    base_demand_total : float\n",
    "        Total base demand for normalization\n",
    "    variation_percentages : list\n",
    "        List of percentage variations from the base demand to simulate\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ENERGY ALLOCATION SIMULATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test with different supply levels\n",
    "    for variation in variation_percentages:\n",
    "        available_supply = base_demand_total * (1 + variation/100)\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"SCENARIO: Supply at {100+variation}% of demand ({available_supply:.2f} MW)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Run the allocation algorithm\n",
    "        allocation = allocate_energy(forecasted_demand, available_supply)\n",
    "        \n",
    "        # Analyze the results\n",
    "        analyze_allocation(allocation, forecasted_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_allocation(allocation, forecasted_demand):\n",
    "    \"\"\"\n",
    "    Visualize the energy allocation results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    allocation : dict\n",
    "        Dictionary mapping feeder names to allocated energy in MW\n",
    "    forecasted_demand : dict\n",
    "        Dictionary mapping feeder names to forecasted demand in MW\n",
    "    \"\"\"\n",
    "    # Create allocation plot directory\n",
    "    alloc_dir = 'allocation_plots/'\n",
    "    if not os.path.exists(alloc_dir):\n",
    "        os.makedirs(alloc_dir)\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    feeders = list(allocation.keys())\n",
    "    allocated = [allocation[f] for f in feeders]\n",
    "    demanded = [forecasted_demand[f] for f in feeders]\n",
    "    \n",
    "    # Calculate allocation percentage\n",
    "    allocation_pct = [allocation[f]/forecasted_demand[f]*100 if forecasted_demand[f] > 0 else 0 for f in feeders]\n",
    "    \n",
    "    # Sort by priority tier\n",
    "    tier_order = {f: i for i, f in enumerate(PRIORITY_ORDER)}\n",
    "    feeders_sorted = sorted(feeders, key=lambda x: tier_order.get(x, 999))\n",
    "    allocated_sorted = [allocation[f] for f in feeders_sorted]\n",
    "    demanded_sorted = [forecasted_demand[f] for f in feeders_sorted]\n",
    "    allocation_pct_sorted = [allocation[f]/forecasted_demand[f]*100 if forecasted_demand[f] > 0 else 0 for f in feeders_sorted]\n",
    "    \n",
    "    # Create a color map for the tiers\n",
    "    tier_colors = {\n",
    "        'SECURITY': 'darkred',\n",
    "        'HEALTHCARE': 'red',\n",
    "        'FINANCIAL': 'orange',\n",
    "        'INDUSTRIAL': 'teal',\n",
    "        'GENERAL': 'gray'\n",
    "    }\n",
    "    \n",
    "    colors = []\n",
    "    for f in feeders_sorted:\n",
    "        if f in TIER1['SECURITY']:\n",
    "            colors.append(tier_colors['SECURITY'])\n",
    "        elif f in TIER1['HEALTHCARE']:\n",
    "            colors.append(tier_colors['HEALTHCARE'])\n",
    "        elif f in TIER2['FINANCIAL']:\n",
    "            colors.append(tier_colors['FINANCIAL'])\n",
    "        elif f in TIER3['INDUSTRIAL']:\n",
    "            colors.append(tier_colors['INDUSTRIAL'])\n",
    "        else:\n",
    "            colors.append(tier_colors['GENERAL'])\n",
    "    \n",
    "    # 1. Allocation vs Demand Bar Chart\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    x = range(len(feeders_sorted))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x, demanded_sorted, width, label='Demand', color='lightgray', edgecolor='black')\n",
    "    plt.bar(x, allocated_sorted, width, label='Allocated', color=colors, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Feeder')\n",
    "    plt.ylabel('Energy (MW)')\n",
    "    plt.title('Energy Allocation vs Demand by Feeder')\n",
    "    plt.xticks(x, feeders_sorted, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add tier-based labels\n",
    "    plt.axvline(x=len(TIER1['SECURITY'])+len(TIER1['HEALTHCARE'])-0.5, color='black', linestyle='--')\n",
    "    plt.axvline(x=len(TIER1['SECURITY'])+len(TIER1['HEALTHCARE'])+len(TIER2['FINANCIAL'])-0.5, color='black', linestyle='--')\n",
    "    plt.axvline(x=len(TIER1['SECURITY'])+len(TIER1['HEALTHCARE'])+len(TIER2['FINANCIAL'])+len(TIER3['INDUSTRIAL'])-0.5, color='black', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(alloc_dir, 'allocation_vs_demand.png'), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Allocation Percentage Bar Chart\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.bar(x, allocation_pct_sorted, color=colors, alpha=0.7)\n",
    "    plt.axhline(y=100, color='red', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Feeder')\n",
    "    plt.ylabel('Allocation Percentage (%)')\n",
    "    plt.title('Energy Allocation Percentage by Feeder')\n",
    "    plt.xticks(x, feeders_sorted, rotation=45)\n",
    "    \n",
    "    # Add tier-based labels\n",
    "    plt.axvline(x=len(TIER1['SECURITY'])+len(TIER1['HEALTHCARE'])-0.5, color='black', linestyle='--')\n",
    "    plt.axvline(x=len(TIER1['SECURITY'])+len(TIER1['HEALTHCARE'])+len(TIER2['FINANCIAL'])-0.5, color='black', linestyle='--')\n",
    "    plt.axvline(x=len(TIER1['SECURITY'])+len(TIER1['HEALTHCARE'])+len(TIER2['FINANCIAL'])+len(TIER3['INDUSTRIAL'])-0.5, color='black', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(alloc_dir, 'allocation_percentage.png'), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Tier-based allocation pie chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    tier_names = ['Security', 'Healthcare', 'Financial', 'Industrial', 'General']\n",
    "    tier_demands = [\n",
    "        sum(forecasted_demand.get(f, 0) for f in TIER1['SECURITY']),\n",
    "        sum(forecasted_demand.get(f, 0) for f in TIER1['HEALTHCARE']),\n",
    "        sum(forecasted_demand.get(f, 0) for f in TIER2['FINANCIAL']),\n",
    "        sum(forecasted_demand.get(f, 0) for f in TIER3['INDUSTRIAL']),\n",
    "        sum(forecasted_demand.get(f, 0) for f in TIER4['GENERAL'])\n",
    "    ]\n",
    "    tier_allocated = [\n",
    "        sum(allocation.get(f, 0) for f in TIER1['SECURITY']),\n",
    "        sum(allocation.get(f, 0) for f in TIER1['HEALTHCARE']),\n",
    "        sum(allocation.get(f, 0) for f in TIER2['FINANCIAL']),\n",
    "        sum(allocation.get(f, 0) for f in TIER3['INDUSTRIAL']),\n",
    "        sum(allocation.get(f, 0) for f in TIER4['GENERAL'])\n",
    "    ]\n",
    "    tier_colors_list = [tier_colors['SECURITY'], tier_colors['HEALTHCARE'], tier_colors['FINANCIAL'], tier_colors['INDUSTRIAL'], tier_colors['GENERAL']]\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.pie(tier_demands, labels=tier_names, autopct='%1.1f%%', colors=tier_colors_list)\n",
    "    plt.title('Demand Distribution by Priority Tier')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie(tier_allocated, labels=tier_names, autopct='%1.1f%%', colors=tier_colors_list)\n",
    "    plt.title('Allocation Distribution by Priority Tier')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(alloc_dir, 'tier_allocation.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6663a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_intelligent_energy_management(model, feature_scaler, target_scaler, last_known_data, feeder_cols, supply_variations=[-20, -10, 0, 10, 20]):\n",
    "    \"\"\"\n",
    "    Run the intelligent energy management system\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Trained LSTM model\n",
    "    feature_scaler : MinMaxScaler\n",
    "        Scaler used for features\n",
    "    target_scaler : MinMaxScaler\n",
    "        Scaler used for targets\n",
    "    last_known_data : np.array\n",
    "        Last known data point to start the forecast\n",
    "    feeder_cols : list\n",
    "        List of feeder column names\n",
    "    supply_variations : list\n",
    "        List of percentage variations from the base demand to simulate\n",
    "    \"\"\"\n",
    "    # Generate forecast for next 24 hours\n",
    "    # For a full year forecast, you would use generate_one_year_forecast() instead\n",
    "    next_day_forecast = model.predict(last_known_data)\n",
    "    \n",
    "    # Convert forecast to original scale\n",
    "    n_outputs = len(feeder_cols)\n",
    "    forecast_orig = target_scaler.inverse_transform(\n",
    "        next_day_forecast.reshape(-1, n_outputs)\n",
    "    ).reshape(next_day_forecast.shape)\n",
    "    \n",
    "    # Calculate total energy demand for each feeder for the next day\n",
    "    daily_demand = {}\n",
    "    for i, feeder in enumerate(feeder_cols):\n",
    "        # Sum across 24 hours to get total daily demand\n",
    "        daily_demand[feeder] = np.sum(forecast_orig[0, :, i])\n",
    "    \n",
    "    # Run the allocation simulation with different supply scenarios\n",
    "    total_demand = sum(daily_demand.values())\n",
    "    sharing_ratios = {feeder: (demand/total_demand*100) for feeder, demand in daily_demand.items()}\n",
    "\n",
    "# Print the sharing ratios in descending order\n",
    "    for feeder, ratio in sorted(sharing_ratios.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"The sharing ratio for {feeder}: {ratio:.2f}%\")\n",
    "    run_allocation_simulation(daily_demand, total_demand, supply_variations)\n",
    "    \n",
    "    # Store allocations for each scenario\n",
    "    allocations = {}\n",
    "    for variation in supply_variations:\n",
    "        available_supply = total_demand * (1 + variation/100)\n",
    "        scenario_name = f\"{100+variation}% of demand ({available_supply:.2f} MW)\"\n",
    "        allocations[scenario_name] = allocate_energy(daily_demand, available_supply)\n",
    "    \n",
    "    # Visualize each scenario (or select specific ones to visualize)\n",
    "    for scenario_name, allocation in allocations.items():\n",
    "        print(f\"\\nVisualizing scenario: {scenario_name}\")\n",
    "        visualize_allocation(allocation, daily_demand)\n",
    "    \n",
    "    # Return the demand and all allocations\n",
    "    return daily_demand, allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Assuming you have the last test sample as last_known_data\n",
    "last_known_data = X_test[[5]]  # First test sample\n",
    "\n",
    "# Run the energy management system\n",
    "daily_demand, allocation = run_intelligent_energy_management(\n",
    "    model,\n",
    "    feature_scaler,\n",
    "    target_scaler,\n",
    "    last_known_data,\n",
    "    feeder_cols,\n",
    "    supply_variations=[-20, -10, 0, 10, 20]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436e91a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
